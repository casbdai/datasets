{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rd7-GCABt8QU"
   },
   "source": [
    "# Hands on - Comparing GradientBoostingClassifier vs. XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IijoUP_TlX7W"
   },
   "source": [
    "Please install xgboost - it is not part of the anaconda framework. Of course, it has to be performed only once :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3691,
     "status": "ok",
     "timestamp": 1631276239797,
     "user": {
      "displayName": "Ivo Blohm",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03588522102684867783"
     },
     "user_tz": -120
    },
    "id": "sjJhn3GolX7X",
    "outputId": "8dea535f-a387-4717-9328-8e659415ce4b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRfhOX7NlX7Z"
   },
   "source": [
    "# Import and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1631276243427,
     "user": {
      "displayName": "Ivo Blohm",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03588522102684867783"
     },
     "user_tz": -120
    },
    "id": "Hxeatr9ao0Il"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "churn = pd.read_csv(\"https://raw.githubusercontent.com/casbdai/datasets/main/churn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SGlLhLneurF"
   },
   "source": [
    "## Separate Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1631276244753,
     "user": {
      "displayName": "Ivo Blohm",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03588522102684867783"
     },
     "user_tz": -120
    },
    "id": "-vkKVc94wLU0"
   },
   "outputs": [],
   "source": [
    "X = churn.drop(\"churn\",axis=1) # Features\n",
    "y = churn[\"churn\"]# Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_Z7kWHvgDe8"
   },
   "source": [
    "## Dummy coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1631276247724,
     "user": {
      "displayName": "Ivo Blohm",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03588522102684867783"
     },
     "user_tz": -120
    },
    "id": "zUIEXuxdfz52"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aFUqgQNr-GV"
   },
   "source": [
    " # Compare GradientBoostingClassifier and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDHmrYFIo0I2"
   },
   "source": [
    "The GradientBoosting Algorithm is one of the most powerful algorithms that we have today. It is very frequently used in research and practice and many winning submissions to kaggle competitions are based on GradientBoosting. In short, Data Scientists love it, because it is fast, accurate, and can be well adapted to many prediction problems.\n",
    "\n",
    "Sklearn offers a \"standard\" implementation of Gradient Boosting Algorithm. However, the same algorithm can be implemented in different software libraries. The software library \"XGBoost\" offers an even more powerful implementation of GradientBoosting - usually Data Scientists use this algorithm when speaking of \"boosting.\" \n",
    "\n",
    "In the following, we want to shortly compare the standard GradientBoostingClassifier() with a XGBoostClassier() on the customer churn data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E20hIjzPki2O"
   },
   "source": [
    "## 1) XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5692,
     "status": "ok",
     "timestamp": 1631276300311,
     "user": {
      "displayName": "Ivo Blohm",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03588522102684867783"
     },
     "user_tz": -120
    },
    "id": "F8qfJ4k6lwaI",
    "outputId": "b96863cd-8680-417b-c39c-88647cf95d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       925\n",
      "           1       0.73      0.84      0.78       534\n",
      "\n",
      "    accuracy                           0.83      1459\n",
      "   macro avg       0.81      0.83      0.82      1459\n",
      "weighted avg       0.84      0.83      0.83      1459\n",
      "\n",
      "CPU times: total: 22.9 s\n",
      "Wall time: 6.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import functions\n",
    "from xgboost import XGBClassifier #we need to import XGBoost instead of GradientBoostingClassifier()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Instantiate the Model\n",
    "clf_xgb = XGBClassifier(learning_rate=0.1,\n",
    "                        n_estimators=500, \n",
    "                        max_depth=10,\n",
    "                        use_label_encoder=False)\n",
    "\n",
    "# Create Test and Training Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Fit Model to Data\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions on Test DAta\n",
    "y_pred = clf_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate Performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8CFkr-So0Jj"
   },
   "source": [
    "# 2) Sklearn's GradientBooostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8965,
     "status": "ok",
     "timestamp": 1631276309261,
     "user": {
      "displayName": "Ivo Blohm",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03588522102684867783"
     },
     "user_tz": -120
    },
    "id": "NywXaUI3o0Jn",
    "outputId": "7e502e3e-95f4-43c6-b0a2-f76f5e7d14be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       925\n",
      "           1       0.77      0.80      0.78       534\n",
      "\n",
      "    accuracy                           0.84      1459\n",
      "   macro avg       0.82      0.83      0.82      1459\n",
      "weighted avg       0.84      0.84      0.84      1459\n",
      "\n",
      "CPU times: total: 17.8 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Import functions\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Instantiate the Model\n",
    "clf_skl = GradientBoostingClassifier(learning_rate=0.1,\n",
    "                                     n_estimators=500, \n",
    "                                     max_depth=10)\n",
    "\n",
    "# Create Test and Training Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Fit Model to Data\n",
    "clf_skl.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions on Test DAta\n",
    "y_pred = clf_skl.predict(X_test)\n",
    "\n",
    "# Evaluate Performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71iG95Loo0Js"
   },
   "source": [
    "# 3) Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7983SLQo0Jt"
   },
   "source": [
    "We overall performance measures in F1-Score are identical (0.78). But there are small differences in Precision and Recall. \n",
    "\n",
    "However, XGBoost is up to two or three times faster (Compare \"Wall time\" values; results depend a bit on your machine and setup)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification XGBoost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
